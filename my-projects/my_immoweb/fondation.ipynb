{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "immoEliza project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libruries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Web Scraping\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class URLsScraperModule:\n",
    "    # ------------------------------------------------------  generating 333 pages\n",
    "    def __init__(self, first_page_url, total_group_pages, save_path_group_urls): \n",
    "        self.first_page_url = first_page_url\n",
    "        self.total_group_pages = total_group_pages\n",
    "        self.save_path = save_path_group_urls\n",
    "        \n",
    "        self.session = requests.Session()  # ایجاد یک Session برای مدیریت کوکی‌ها\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }  # هدر برای شبیه‌سازی درخواست از مرورگر واقعی\n",
    "\n",
    "    def generate_page_links(self):\n",
    "        \"\"\"\n",
    "        ایجاد لینک‌ها برای صفحات و ذخیره در فایل CSV.\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        for page_num in range(1, self.total_group_pages + 1):\n",
    "            page_group_utls = f\"{self.first_page_url}&page={page_num}\"\n",
    "            links.append(page_group_utls)\n",
    "        \n",
    "        df = pd.DataFrame(links, columns=['page_group_utls'])\n",
    "        df.to_csv(self.save_path, index=False)\n",
    "              \n",
    "        print(f\"لینک‌های صفحات 1 تا {self.total_group_pages} در فایل {self.save_path} ذخیره شدند.\")\n",
    "\n",
    "    # ------------------------------------------------------ extract all links from a limited number of pages\n",
    "    def extract_item_links(self, save_path_all_urls, limit=None):\n",
    "        \"\"\"\n",
    "        استخراج لینک‌های آیتم‌های فروش از صفحات و ذخیره در فایل CSV.\n",
    "        پارامتر `limit` برای تعیین تعداد صفحات استفاده می‌شود (مقدار None یعنی همه صفحات).\n",
    "        \"\"\"\n",
    "        # خواندن لینک‌های صفحات از فایل CSV\n",
    "        page_links_df = pd.read_csv(self.save_path)\n",
    "        all_urls = []\n",
    "\n",
    "        # اگر محدودیتی تعیین شده باشد، آن تعداد از صفحات انتخاب می‌شود\n",
    "        if limit is not None:\n",
    "            page_links_df = page_links_df.head(limit)\n",
    "\n",
    "        # اسکرپ کردن از هر صفحه\n",
    "        for page_url in page_links_df['page_group_utls']:\n",
    "            try:\n",
    "                response = self.session.get(page_url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # استخراج لینک‌های آیتم‌ها\n",
    "                for a_tag in soup.find_all('a', class_='card__title-link', href=True):\n",
    "                    all_urls.append(a_tag['href'])\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"خطا در دسترسی به صفحه {page_url}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # ذخیره لینک‌های آیتم‌ها در یک فایل CSV\n",
    "        df = pd.DataFrame(all_urls, columns=['all_urls'])\n",
    "        df.to_csv(save_path_all_urls, index=False)\n",
    "        print(f\"لینک‌های آیتم‌های فروش در فایل {save_path_all_urls} ذخیره شدند.\")\n",
    "\n",
    "\n",
    "# استفاده از کلاس و تولید لینک‌ها\n",
    "first_page_url = \"https://www.immoweb.be/en/search/house-and-apartment/for-sale?countries=BE&orderBy=relevance\"\n",
    "total_group_pages = 333\n",
    "save_path_group_urls = \"C:/Users/becod/AI/my-projects/my_immoweb/page_group_utls.csv\"\n",
    "\n",
    "scraper = URLsScraperModule(first_page_url, total_group_pages, save_path_group_urls)\n",
    "scraper.generate_page_links()\n",
    "\n",
    "# برای تست می‌توانید تعداد صفحات مورد نظر را تعیین کنید\n",
    "save_path_all_urls = \"C:/Users/becod/AI/my-projects/my_immoweb/all_urls.csv\"\n",
    "limit = 332                                                    # ///// to how many times run (limit is 1 - 333)\n",
    "scraper.extract_item_links(save_path_all_urls, limit)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning, step 1 (cleaning URLs, before scraping properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned URLs saved to C:/Users/becod/AI/my-projects/my_immoweb/all_urls_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# URLs Cleaner Module\n",
    "\n",
    "class URLsCleanerModule:\n",
    "    \n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    def __init__(self, input_file, output_file):\n",
    "        \"\"\"\n",
    "        Initialize the URLsCleanerModule with input and output file paths.\n",
    "        \"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.df = None\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    # Function to read URLs from a CSV file\n",
    "    def read_urls(self):\n",
    "        \"\"\"\n",
    "        Reads URLs from a CSV file and loads them into a DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.input_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {self.input_file}: {e}\")\n",
    "            self.df = None\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    # 1. Clean unnecessary whitespace from URLs\n",
    "    def clean_whitespace(self):\n",
    "        \"\"\"\n",
    "        Clean unnecessary whitespace from URLs in the DataFrame.\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            self.df['all_urls'] = self.df['all_urls'].str.strip()  # Strip leading/trailing whitespace\n",
    "        return self\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    # 2. Check for broken URLs (validate format)\n",
    "    def validate_urls(self):\n",
    "        \"\"\"\n",
    "        Keep only valid URLs that start with 'http://' or 'https://'.\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            self.df = self.df[self.df['all_urls'].str.startswith(('http://', 'https://'))]\n",
    "        return self\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    # 3. Remove URLs containing \"new-real-estate-project-apartments\" or \"new-real-estate-project-houses\"\n",
    "    def remove_new_real_estate_projects(self):\n",
    "        \"\"\"\n",
    "        Remove URLs containing 'new-real-estate-project-apartments' or 'new-real-estate-project-houses'.\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            self.df = self.df[~self.df['all_urls'].str.contains('new-real-estate-project-(?:apartments|houses)', case=False, regex=True)]\n",
    "        return self\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    # 4. Remove duplicates from the URL dataset\n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"\n",
    "        Remove duplicate URLs from the dataset.\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            self.df = self.df.drop_duplicates()\n",
    "        return self\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    # Function to save cleaned URLs to a CSV file\n",
    "    def save_cleaned_urls(self):\n",
    "        \"\"\"\n",
    "        Save the cleaned URLs to a CSV file.\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            try:\n",
    "                self.df.to_csv(self.output_file, index=False)\n",
    "                print(f\"Cleaned URLs saved to {self.output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving file {self.output_file}: {e}\")\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    \n",
    "    # Main function for cleaning the URL dataset\n",
    "    def clean_urls(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the entire URL cleaning process step by step.\n",
    "        \"\"\"\n",
    "        self.read_urls()  # Read the URLs from the CSV file\n",
    "        if self.df is None:\n",
    "            return\n",
    "        \n",
    "        # Apply each cleaning function in sequence\n",
    "        self.clean_whitespace()\\\n",
    "            .validate_urls()\\\n",
    "            .remove_new_real_estate_projects()\\\n",
    "            .remove_duplicates()\\\n",
    "            .save_cleaned_urls()\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# Example usage: To run the cleaning process\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"C:/Users/becod/AI/my-projects/my_immoweb/all_urls.csv\"\n",
    "    output_file = \"C:/Users/becod/AI/my-projects/my_immoweb/all_urls_cleaned.csv\"\n",
    "    \n",
    "    # Create an instance of the URLsCleanerModule and run the cleaning pipeline\n",
    "    cleaner = URLsCleanerModule(input_file, output_file)\n",
    "    cleaner.clean_urls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape all properties from all cleaned URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#_____________________________________________________________________\n",
    "\n",
    "class ScraperModule:\n",
    "    # Function to get the soup object from the webpage URLsCleanerModule\n",
    "    def get_soup(self, url):\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return soup\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching the URL {url}: {e}\")\n",
    "            return None\n",
    "    #_____________________________________________________________________\n",
    "    # extract Property ID from URL\n",
    "    def get_property_id(self, url):\n",
    "        try:\n",
    "            return url.split('/')[-1]  # Last part of the URL\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract Postal Code from URL\n",
    "    def get_postal_code(self, url):\n",
    "        try:\n",
    "            return url.split('/')[-2]  # Second-to-last part of the URL\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract Locality from URL\n",
    "    def get_locality(self, url):\n",
    "        try:\n",
    "            return url.split('/')[-3]  # Third-to-last part of the URL\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract Sale Type from URL\n",
    "    def get_sale_type(self, url):\n",
    "        try:\n",
    "            return url.split('/')[-4]  # Fourth-to-last part of the URL\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract Property Type from URL\n",
    "    def get_property_type(self, url):\n",
    "        try:\n",
    "            return url.split('/')[-5]  # Fifth-to-last part of the URL\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    #  extract  address\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract price \n",
    "    def get_price(self, soup):\n",
    "        try:\n",
    "            price_element = soup.find('p', class_='classified__price')\n",
    "            if price_element:\n",
    "                price_value = price_element.get_text(strip=True)\n",
    "\n",
    "                # Check if there's a price range (min - max)\n",
    "                if '-' in price_value:\n",
    "                    min_price, max_price = price_value.split('-')\n",
    "                    min_price = ''.join(filter(str.isdigit, min_price))  # Extract only digits for the min price\n",
    "                    max_price = ''.join(filter(str.isdigit, max_price))  # Extract only digits for the max price\n",
    "                    return f\"{min_price} - {max_price}\"  # Return the price range\n",
    "                else:\n",
    "                    # If there's only a single price\n",
    "                    price = ''.join(filter(str.isdigit, price_value))  # Only keep digits\n",
    "                    return price if price else \"None\"\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "\n",
    "    #_____________________________________________________________________\n",
    "    # extract living area \n",
    "    def get_living_area(self, soup):\n",
    "        try:\n",
    "            living_area_row = soup.find('th', string=lambda text: text and \"Living area\" in text)\n",
    "            if living_area_row:\n",
    "                living_area_value_tag = living_area_row.find_next('td', class_='classified-table__data')\n",
    "                if living_area_value_tag:\n",
    "                    for content in living_area_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the extracted number\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract  number of bedrooms\n",
    "    def get_bedrooms(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Bedrooms\"\n",
    "            bedrooms_row = soup.find('th', string=lambda text: text and \"Bedrooms\" in text)\n",
    "            if bedrooms_row:\n",
    "                bedrooms_value_tag = bedrooms_row.find_next('td', class_='classified-table__data')\n",
    "                if bedrooms_value_tag:\n",
    "                    # Loop through the contents of the tag to find the numeric value\n",
    "                    for content in bedrooms_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the extracted number\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract  number of bathrooms\n",
    "    def get_bathrooms(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Bathrooms\"\n",
    "            bathrooms_row = soup.find('th', string=lambda text: text and \"Bathrooms\" in text)\n",
    "            if bathrooms_row:\n",
    "                bathrooms_value_tag = bathrooms_row.find_next('td', class_='classified-table__data')\n",
    "                if bathrooms_value_tag:\n",
    "                    # Loop through the contents of the tag to find the numeric value\n",
    "                    for content in bathrooms_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the extracted number\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract  kitchen type\n",
    "    def get_kitchen_type(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Kitchen type\"\n",
    "            kitchen_row = soup.find('th', string=lambda text: text and \"Kitchen type\" in text)\n",
    "            if kitchen_row:\n",
    "                kitchen_value_tag = kitchen_row.find_next('td', class_='classified-table__data')\n",
    "                if kitchen_value_tag:\n",
    "                    # Loop through the contents to find the kitchen type (text string)\n",
    "                    for content in kitchen_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():\n",
    "                            return content.strip()  # Return the kitchen type text\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract  building condition \n",
    "    def get_building_condition(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Building condition\"\n",
    "            condition_row = soup.find('th', string=lambda text: text and \"Building condition\" in text)\n",
    "            if condition_row:\n",
    "                condition_value_tag = condition_row.find_next('td', class_='classified-table__data')\n",
    "                if condition_value_tag:\n",
    "                    # Loop through the contents to find the building condition (text string)\n",
    "                    for content in condition_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():\n",
    "                            return content.strip()  # Return the building condition text\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "\n",
    "    #_____________________________________________________________________\n",
    "    # extract  number of frontages \n",
    "    def get_number_of_frontages(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Number of frontages\"\n",
    "            frontages_row = soup.find('th', string=lambda text: text and \"Number of frontages\" in text)\n",
    "            if frontages_row:\n",
    "                frontages_value_tag = frontages_row.find_next('td', class_='classified-table__data')\n",
    "                if frontages_value_tag:\n",
    "                    # Loop through the contents to find the frontages number (text string)\n",
    "                    for content in frontages_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the number of frontages\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract  number of floors \n",
    "    def get_number_of_floors(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Number of floors\"\n",
    "            floors_row = soup.find('th', string=lambda text: text and \"Number of floors\" in text)\n",
    "            if floors_row:\n",
    "                floors_value_tag = floors_row.find_next('td', class_='classified-table__data')\n",
    "                if floors_value_tag:\n",
    "                    # Loop through the contents to find the floors number (text string)\n",
    "                    for content in floors_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the number of floors\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "\n",
    "    #_____________________________________________________________________\n",
    "    # extract floor number \n",
    "    def get_floor(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Floor\"\n",
    "            floor_row = soup.find('th', string=lambda text: text and \"Floor\" in text)\n",
    "            if floor_row:\n",
    "                floor_value_tag = floor_row.find_next('td', class_='classified-table__data')\n",
    "                if floor_value_tag:\n",
    "                    # Loop through the contents to find the floor number (text string)\n",
    "                    for content in floor_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the floor number\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "\n",
    "    #_____________________________________________________________________\n",
    "    #  extract construction year\n",
    "    def get_construction_year(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Construction year\"\n",
    "            year_row = soup.find('th', string=lambda text: text and \"Construction year\" in text)\n",
    "            if year_row:\n",
    "                year_value_tag = year_row.find_next('td', class_='classified-table__data')\n",
    "                if year_value_tag:\n",
    "                    # Loop through the contents to find the year (text string)\n",
    "                    for content in year_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the construction year\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"   \n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # //////////  Interior\n",
    "    # Fextract  Furnished status\n",
    "    def get_furnished(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Furnished\"\n",
    "            furnished_row = soup.find('th', string=lambda text: text and \"Furnished\" in text)\n",
    "            if furnished_row:\n",
    "                furnished_value_tag = furnished_row.find_next('td', class_='classified-table__data')\n",
    "                if furnished_value_tag:\n",
    "                    # Loop through the contents of the tag to find the furnished status\n",
    "                    for content in furnished_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned furnished status\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "\n",
    "    #_____________________________________________________________________\n",
    "    # Function to extract the Basement information from the soup object\n",
    "    def get_basement(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Basement\"\n",
    "            basement_row = soup.find('th', string=lambda text: text and \"Basement\" in text)\n",
    "            if basement_row:\n",
    "                basement_value_tag = basement_row.find_next('td', class_='classified-table__data')\n",
    "                if basement_value_tag:\n",
    "                    # Loop through the contents of the tag to find the basement value\n",
    "                    for content in basement_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned basement value\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________    \n",
    "    # //////////  Exterior\n",
    "    # extract  Surface of the plot\n",
    "    def get_surface_of_plot(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Surface of the plot\"\n",
    "            surface_row = soup.find('th', string=lambda text: text and \"Surface of the plot\" in text)\n",
    "            if surface_row:\n",
    "                surface_value_tag = surface_row.find_next('td', class_='classified-table__data')\n",
    "                if surface_value_tag:\n",
    "                    # Loop through the contents of the tag to find the surface value\n",
    "                    for content in surface_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned surface value\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________  \n",
    "    # extract  Garden surface\n",
    "    def get_garden_surface(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Garden surface\"\n",
    "            garden_surface_row = soup.find('th', string=lambda text: text and \"Garden surface\" in text)\n",
    "            if garden_surface_row:\n",
    "                garden_surface_value_tag = garden_surface_row.find_next('td', class_='classified-table__data')\n",
    "                if garden_surface_value_tag:\n",
    "                    # Loop through the contents of the tag to find the garden surface value\n",
    "                    for content in garden_surface_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned garden surface value\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract  Terrace surface\n",
    "    def get_terrace_surface(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Terrace surface\"\n",
    "            terrace_surface_row = soup.find('th', string=lambda text: text and \"Terrace surface\" in text)\n",
    "            if terrace_surface_row:\n",
    "                terrace_surface_value_tag = terrace_surface_row.find_next('td', class_='classified-table__data')\n",
    "                if terrace_surface_value_tag:\n",
    "                    # Loop through the contents of the tag to find the terrace surface value\n",
    "                    for content in terrace_surface_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned terrace surface value\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________ \n",
    "    # //////////  Facilities\n",
    "    # extract  Elevator status\n",
    "    def get_elevator(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Elevator\"\n",
    "            elevator_row = soup.find('th', string=lambda text: text and \"Elevator\" in text)\n",
    "            if elevator_row:\n",
    "                elevator_value_tag = elevator_row.find_next('td', class_='classified-table__data')\n",
    "                if elevator_value_tag:\n",
    "                    # Loop through the contents of the tag to find the elevator status\n",
    "                    for content in elevator_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned elevator status\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract Swimming pool status\n",
    "    def get_swimming_pool(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Swimming pool\"\n",
    "            swimming_pool_row = soup.find('th', string=lambda text: text and \"Swimming pool\" in text)\n",
    "            if swimming_pool_row:\n",
    "                swimming_pool_value_tag = swimming_pool_row.find_next('td', class_='classified-table__data')\n",
    "                if swimming_pool_value_tag:\n",
    "                    # Loop through the contents of the tag to find the swimming pool status\n",
    "                    for content in swimming_pool_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned swimming pool status\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # //////////  Energy\n",
    "    # extract  primary energy consumption \n",
    "    def get_primary_energy_consumption(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Primary energy consumption\"\n",
    "            energy_row = soup.find('th', string=lambda text: text and \"Primary energy consumption\" in text)\n",
    "            if energy_row:\n",
    "                energy_value_tag = energy_row.find_next('td', class_='classified-table__data')\n",
    "                if energy_value_tag:\n",
    "                    # Loop through the contents to find the value (text string)\n",
    "                    for content in energy_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip().isdigit():\n",
    "                            return content.strip()  # Return the energy consumption value\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # extract  Energy class\n",
    "    def get_energy_class(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Energy class\"\n",
    "            energy_class_row = soup.find('th', string=lambda text: text and \"Energy class\" in text)\n",
    "            if energy_class_row:\n",
    "                energy_class_value_tag = energy_class_row.find_next('td', class_='classified-table__data')\n",
    "                if energy_class_value_tag:\n",
    "                    # Loop through the contents of the tag to find the energy class value\n",
    "                    for content in energy_class_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned energy class value\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    #/////////////////////////////////\n",
    "    # extract Flood zone type \n",
    "    def get_flood_zone_type(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Flood zone type\"\n",
    "            flood_zone_row = soup.find('th', string=lambda text: text and \"Flood zone type\" in text)\n",
    "            if flood_zone_row:\n",
    "                flood_zone_value_tag = flood_zone_row.find_next('td', class_='classified-table__data')\n",
    "                if flood_zone_value_tag:\n",
    "                    # Loop through the contents of the tag to find the flood zone type value\n",
    "                    for content in flood_zone_value_tag.contents:\n",
    "                        if isinstance(content, str) and content.strip():  # Check if the content is a string and not empty\n",
    "                            return content.strip()  # Return the cleaned flood zone type value\n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    #/////////////////////////////////\n",
    "    # extract Public sale \n",
    "    def get_public_sale(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains \"Venue of the sale\"\n",
    "            venue_row = soup.find('th', string=lambda text: text and \"Venue of the sale\" in text)\n",
    "            if venue_row:\n",
    "                venue_value_tag = venue_row.find_next('td', class_='classified-table__data')\n",
    "                if venue_value_tag:\n",
    "                    # Check if there's a link indicating it's a public sale\n",
    "                    link_tag = venue_value_tag.find(['a', 'iw-tracked-link'], href=True)\n",
    "                    if link_tag:\n",
    "                        return \"Public sale\"\n",
    "            return \"Not a public sale\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    #  extract price label \n",
    "    def get_price_label(self, soup):\n",
    "        try:\n",
    "            # Search for the section that contains the price label\n",
    "            price_label_row = soup.find('p', class_='classified__price')\n",
    "            if price_label_row:\n",
    "                # Extract the text that could contain \"Starting price\" or \"Make an offer\"\n",
    "                price_label = price_label_row.find('span', class_='classified__price-label')\n",
    "                if price_label:\n",
    "                    price_label_text = price_label.get_text(strip=True)\n",
    "\n",
    "                    # Check if the label contains \"Starting price\"\n",
    "                    if \"Starting price\" in price_label_text:\n",
    "                        return \"Starting price\"\n",
    "\n",
    "                    # Check if the label contains \"Make an offer\"\n",
    "                    if \"Make an offer\" in price_label_text:\n",
    "                        return \"Make an offer\"              \n",
    "            return \"None\"\n",
    "        except Exception as e:\n",
    "            return \"None\"\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    #_____________________________________________________________________\n",
    "    #_____________________________________________________________________\n",
    "    # Function to process a single URL and extract all properties\n",
    "    def scrape_property(self, url):\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:  # If the page could not be fetched, return None\n",
    "            return None\n",
    "\n",
    "        property_details = {\n",
    "            'Property ID': self.get_property_id(url),\n",
    "            'Postal Code': self.get_postal_code(url),\n",
    "            'Locality': self.get_locality(url),\n",
    "            #'Address': self.get_address(url),\n",
    "            'Property Type': self.get_property_type(url),\n",
    "            'Sale Type': self.get_sale_type(url),\n",
    "            'Price (€)': self.get_price(soup),\n",
    "            'Surface of the plot (m²)': self.get_surface_of_plot(soup),\n",
    "            'Living Area (m²)': self.get_living_area(soup),      \n",
    "            'Bedrooms': self.get_bedrooms(soup),  \n",
    "            'Bathrooms': self.get_bathrooms(soup),\n",
    "            'Furnished': self.get_furnished(soup),\n",
    "            'Kitchen Type': self.get_kitchen_type(soup),\n",
    "            'Building condition': self.get_building_condition(soup),\n",
    "            'number of Frontages': self.get_number_of_frontages(soup),\n",
    "            'number of Floors': self.get_number_of_floors(soup),\n",
    "            'Floor': self.get_floor(soup),\n",
    "            'Construction Year': self.get_construction_year(soup),\n",
    "            'Primary Energy Consumption (kWh/m²)': self.get_primary_energy_consumption(soup),\n",
    "            'Energy class': self.get_energy_class(soup),\n",
    "            'Flood Zone Type': self.get_flood_zone_type(soup),\n",
    "            'Garden Surface': self.get_garden_surface(soup),\n",
    "            'Terrace Surface': self.get_terrace_surface(soup),\n",
    "            'Swimming Pool': self.get_swimming_pool(soup),\n",
    "            'Elevator': self.get_elevator(soup), \n",
    "            'Basement': self.get_basement(soup),\n",
    "            'Public Sale': self.get_public_sale(soup),\n",
    "            'Price Label': self.get_price_label(soup),\n",
    "            'URL': url  # Saving the URL with each property    \n",
    "        } \n",
    "        return property_details\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # Function to save property details to CSV\n",
    "    def save_to_csv(self, property_list, filename):\n",
    "        df = pd.DataFrame(property_list)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    def read_urls_from_csv(self, file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return df[df.columns[0]].tolist()  # This assumes the first column contains URLs\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # handle scraping for multiple URLs\n",
    "    def scrape_multiple_properties(self, urls):\n",
    "        all_properties = []\n",
    "\n",
    "        for url in urls:\n",
    "            property_details = self.scrape_property(url)\n",
    "\n",
    "            if property_details:  # Only add the property details if they are valid\n",
    "                all_properties.append(property_details)\n",
    "\n",
    "            # Add a small delay to prevent rate-limiting\n",
    "            time.sleep(1)  # 1-second delay between requests\n",
    "\n",
    "        return all_properties\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # Main execution\n",
    "    def main(self):\n",
    "        # File path for the CSV containing URLs\n",
    "        url_file_path = \"C:/Users/becod/AI/my-projects/my_immoweb/all_urls_cleaned.csv\"\n",
    "\n",
    "        # Reading the URLs from the CSV file\n",
    "        urls = self.read_urls_from_csv(url_file_path)\n",
    "\n",
    "        if not urls:\n",
    "            print(\"No URLs found to process.\")\n",
    "            return\n",
    "\n",
    "        # Scrape details for all URLs\n",
    "        property_data = self.scrape_multiple_properties(urls)\n",
    "\n",
    "        # Save the scraped data to a CSV file\n",
    "        output_file_path = \"C:/Users/becod/AI/my-projects/my_immoweb/property_details.csv\"\n",
    "        self.save_to_csv(property_data, output_file_path)\n",
    "\n",
    "#_____________________________________________________________________\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ScraperModule()\n",
    "    scraper.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file C:/Users/becod/AI/my-projects/my_immoweb/all_urls_cleaned.csv: name 'pd' is not defined\n",
      "No URLs found to process.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning, step 2 (Cleaning Properties before Analyzing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:/Users/becod/AI/my-projects/my_immoweb/all_results_cleanedup.csv\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Properties dataset\n",
    "\n",
    "class PropertiesCleanerModule:\n",
    "\n",
    "    # Reads a CSV file and returns a pandas DataFrame\n",
    "    def read_csv(self, file_path):       \n",
    "        if os.path.exists(file_path):\n",
    "            return pd.read_csv(file_path)\n",
    "        else:\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return None\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    # 1. remove duplicate rows\n",
    "    def remove_duplicates(self, data):\n",
    "        if data.empty:\n",
    "            print(\"Data is empty. Skipping duplicate removal.\")\n",
    "            return data\n",
    "        return data.drop_duplicates()\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    # 2. remove rows with same properties and different 'Property ID' and 'URL' (remove 2 times registered items)\n",
    "    def remove_duplicate_properties(self, data):\n",
    "        # Check if 'Property ID' and 'URL' columns exist\n",
    "        if 'Property ID' not in data.columns or 'URL' not in data.columns:\n",
    "            print(\"'Property ID' or 'URL' column not found.\")\n",
    "            return data    \n",
    "        # Define columns to compare, excluding 'Property ID' and 'URL'\n",
    "        columns_to_compare = [col for col in data.columns if col not in ['Property ID', 'URL']]  \n",
    "        # Find rows where all columns except 'Property ID' and 'URL' are duplicates\n",
    "        duplicate_mask = data.duplicated(subset=columns_to_compare, keep=False)    \n",
    "        # Keep only one row for each group of duplicates, regardless of 'Property ID' and 'URL'\n",
    "        data_cleaned = data[~duplicate_mask | data.duplicated(subset=columns_to_compare, keep='first')]   \n",
    "        return data_cleaned\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    # 3. remove rows if in column 'Public Sale' value is 'Public sale'    \n",
    "    def remove_public_sale(self, data):\n",
    "        if 'Public Sale' not in data.columns:\n",
    "            print(\"'Public Sale' column not found.\")\n",
    "            return data  \n",
    "        # Remove rows where 'Public Sale' column has the value 'Public sale'\n",
    "        return data[~data['Public Sale'].isin(['Public sale'])]\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "\n",
    "    # 4. remove rows if 'Price Label' value is \"Starting price\" or \"Make an offer\"\n",
    "    def remove_price_label(self, data):\n",
    "        # Check if 'Price Label' column exists\n",
    "        if 'Price Label' not in data.columns:\n",
    "            print(\"'Price Label' column not found.\")\n",
    "            return data \n",
    "        # Remove rows where 'Price Label' is \"Starting price\" or \"Make an offer\"\n",
    "        return data[~data['Price Label'].isin(['Starting price', 'Make an offer'])]\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "    # 3.1 Clean individual 'Locality name' by removing extra characters\n",
    "    def clean_locality_name(self, name):\n",
    "        name = name.replace('%C', 'e')               # replace  %C with  e (originally è)\n",
    "        name = name.replace('%27', \"'\")              # replace  %27 with  '\n",
    "        name = name.replace('%20', ' ')              # replace  %20 with  space\n",
    "        name = re.sub(r'%20%28.*?%29', '', name)     # remove characters between %20%28 and %29\n",
    "        name = re.sub(r'[0-9\\(\\)%]', '', name)       # remove any numbers and special characters\n",
    "        name = re.sub(r'-\\d+$', '', name)            # remove trailing numbers and hyphens\n",
    "        name = name.lower().title()                  # lowercase, capitalize each word\n",
    "        return name.strip()                          # strip extra spaces\n",
    "\n",
    "    # 3.2 Apply the cleaning function to the 'Locality' column in the DataFrame\n",
    "    def clean_locality_column(self, df):\n",
    "        if 'Locality' in df.columns:\n",
    "            df['Locality'] = df['Locality'].apply(self.clean_locality_name)  # Apply to each value in 'Locality'\n",
    "        else:\n",
    "            print(\"'Locality' column not found.\")\n",
    "        return df\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "\n",
    "    # save the cleaned DataFrame to a CSV file\n",
    "    def save_to_csv(self, data, output_path):\n",
    "        if data is not None and not data.empty:\n",
    "            data.to_csv(output_path, index=False)\n",
    "            print(f\"Data saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No data to save.\")\n",
    "\n",
    "    # __________________________________________________________________________\n",
    "\n",
    "    # main function to orchestrate the cleaning process\n",
    "    def clean_data_pipeline(self, input_file, output_file):\n",
    "\n",
    "        # Read the CSV file\n",
    "        data = self.read_csv(input_file)\n",
    "\n",
    "        # Check if data is available\n",
    "        if data is None:\n",
    "            print(\"No data found, exiting the pipeline.\")\n",
    "            return\n",
    "\n",
    "        data_cleaned = self.remove_duplicates(data)\n",
    "        data_cleaned = self.remove_duplicate_properties(data_cleaned)\n",
    "        data_cleaned = self.remove_public_sale(data_cleaned) \n",
    "        data_cleaned = self.remove_price_label(data_cleaned) \n",
    "        data_cleaned = self.clean_locality_column(data_cleaned)\n",
    "\n",
    "        # Save the cleaned data to a new CSV file\n",
    "        self.save_to_csv(data_cleaned, output_file)\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# Example usage: To run the cleaning process\n",
    "if __name__ == \"__main__\":\n",
    "    cleaner = PropertiesCleanerModule()\n",
    "    input_file_path = \"C:/Users/becod/AI/my-projects/my_immoweb/property_details.csv\"\n",
    "    output_file_path = \"C:/Users/becod/AI/my-projects/my_immoweb/all_results_cleanedup.csv\"\n",
    "    \n",
    "    # Run the cleaning pipeline\n",
    "    cleaner.clean_data_pipeline(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_page_html(item_url):\n",
    "    \"\"\"\n",
    "    این متد برای چاپ HTML کامل یک صفحه استفاده می‌شود.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(item_url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # چاپ HTML کامل صفحه\n",
    "        print(response.text)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"خطا در دسترسی به آیتم {item_url}: {str(e)}\")\n",
    "\n",
    "\n",
    "# استفاده از تابع برای چاپ HTML یک صفحه\n",
    "item_url = \"https://www.immoweb.be/en/classified/house/for-sale/schaerbeek/1030/20260393\"  # مثال از یک URL آیتم\n",
    "print_page_html(item_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperModule:\n",
    "    def __init__(self, base_url):\n",
    "        \"\"\"\n",
    "        این متد سازنده (constructor) است که URL وب‌سایت را ذخیره کرده و یک session برای مدیریت کوکی‌ها و درخواست‌ها ایجاد می‌کند.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url  # آدرس وب‌سایت\n",
    "        self.session = None  # مدیریت کوکی‌ها و درخواست‌ها\n",
    "\n",
    "    def access_site(self):\n",
    "        \"\"\"\n",
    "        این متد مسئول دسترسی به سایت، مدیریت کوکی‌ها و خطاها است.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def extract_item_links(self):\n",
    "        \"\"\"\n",
    "        این متد لینک‌های مربوط به آیتم‌های فروش را استخراج کرده و در یک فایل ذخیره می‌کند.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def extract_item_details(self):\n",
    "        \"\"\"\n",
    "        این متد جزئیات آیتم‌ها را از لینک‌های ذخیره شده استخراج کرده و در یک فایل ذخیره می‌کند.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# from scipy import stats\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def remove_nulls(self):\n",
    "        \"\"\"Removes rows with null or invalid values.\"\"\"\n",
    "        # Code to clean data\n",
    "\n",
    "    def normalize_data(self):\n",
    "        \"\"\"Normalizes and standardizes data (e.g., convert units).\"\"\"\n",
    "        # Code to normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis & Visualization module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalyzer:\n",
    "    def __init__(self, cleaned_data):\n",
    "        self.cleaned_data = cleaned_data\n",
    "\n",
    "    def analyze(self):\n",
    "        \"\"\"Performs data analysis and returns insights.\"\"\"\n",
    "        # Code to analyze the data\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Generates charts and plots based on the analysis.\"\"\"\n",
    "        # Code to generate charts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price Index module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Development module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All links saved in C:\\Users\\becod\\AI\\my-projects\\Majid_immoeliza_cleanup\\Links/all_links.csv\n",
      "the links saved in csv file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#_____________________________________________________________________________________________\n",
    "# extracting links\n",
    "\n",
    "# generate the group links (for 333 pages or less)\n",
    "def generate_page_links(base_link, total_pages):\n",
    "    page_links = [base_link]\n",
    "    for p in range(2, total_pages + 1):\n",
    "        page_links.append(base_link + \"&page=\" + str(p))\n",
    "    return page_links\n",
    "\n",
    "\n",
    "# extract all the linkes items for sale \n",
    "def extract_links_from_page(session, page_link):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = session.get(page_link, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    links = []\n",
    "    for tag_a in soup.find_all('a', class_=\"card__title-link\", href=True):\n",
    "        links.append(tag_a['href'])\n",
    "    return links\n",
    "\n",
    "# filter \"new real estate project\" links\n",
    "def filter_links(links):\n",
    "    return [link for link in links if \"/new-real-estate-project-\" not in link]\n",
    "\n",
    "# save the links in a csv file\n",
    "def save_links_to_csv(links, file_path):    # save the files in only 1 file\n",
    "    df = pd.DataFrame(links, columns=['URL'])\n",
    "    file_name = f'{file_path}/all_links.csv'  \n",
    "    df.to_csv(file_name, index=False, encoding='utf-8')\n",
    "    print(f\"All links saved in {file_name}\")\n",
    "\n",
    "# def save_links_to_csv(links, file_path, num_files):   # save the files in 10 file\n",
    "#     links_per_file = math.ceil(len(links) / num_files)\n",
    "    \n",
    "#     for i in range(num_files):\n",
    "#         start_index = i * links_per_file\n",
    "#         end_index = min((i + 1) * links_per_file, len(links))\n",
    "#         links_chunk = links[start_index:end_index]\n",
    "\n",
    "#         df = pd.DataFrame(links_chunk, columns=['URL'])\n",
    "#         file_name = f'{file_path}/links_{i+1}.csv'\n",
    "#         df.to_csv(file_name, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "# calling functions to extract links\n",
    "\n",
    "base_link = 'https://www.immoweb.be/en/search/house-and-apartment/for-sale?countries=BE&amp%3BorderBy=relevance&amp%3Bpage=2'\n",
    "total_pages = 333\n",
    "\n",
    "# calling to generate 333 grope page links\n",
    "page_links = generate_page_links(base_link, total_pages)\n",
    "\n",
    "# extract the links of sale items from 333 group pages (extracting +10000 links)\n",
    "session = requests.Session()\n",
    "all_links = []\n",
    "for page_link in page_links[:3]:  # we can choose wor on 333 pages or less  ///////////////////////////////////\n",
    "    all_links.extend(extract_links_from_page(session, page_link))\n",
    "\n",
    "# calling to filter the \"new real estate project\" links\n",
    "filtered_links = filter_links(all_links)\n",
    "\n",
    "# calling to save the extracted links to a csv file\n",
    "all_saved_linked = save_links_to_csv(filtered_links, 'C:\\\\Users\\\\becod\\\\AI\\\\my-projects\\\\Majid_immoeliza_cleanup\\\\Links')\n",
    "\n",
    "print(\"the links saved in csv file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________\n",
    "#  Extracting detailes\n",
    "\n",
    "# request to URL to extract tht HTML content\n",
    "def get_soup(url):\n",
    "    headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\"}\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "# extracting Property ID\n",
    "def get_property_id(soup):\n",
    "    try:\n",
    "        html = soup.find(\"meta\", {'property': \"og:url\"}).get('content')\n",
    "        html_list = html.split(\"/\")\n",
    "        print(\"code is running\")\n",
    "        return html_list[-1]\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in Property_ID: {e}\")\n",
    "        return None\n",
    "    \n",
    "# extracting Postal code\n",
    "def get_postal_code(html_list):\n",
    "    try:\n",
    "        return html_list[-2]\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in postal_code : {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting Locality (city)\n",
    "def get_locality(html_list):\n",
    "    try:\n",
    "        return html_list[-3]\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in locality : {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting price as string\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        home_meta_info = soup.find_all(\"div\", {'class': 'grid__item desktop--9'})\n",
    "        price = home_meta_info[0].find(\"p\", {'class': 'classified__price'}).find_all('span', {'class':'sr-only'})[0].text.strip()\n",
    "        price = re.sub(r'[^\\d.,\\-+]', '', price)\n",
    "        return str(price)    # convert to str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in price : {e}\")\n",
    "        return None\n",
    "\n",
    "# Type of property\n",
    "def get_property_type(html_list):\n",
    "    try:\n",
    "        return html_list[-5]\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in Type_of_property: {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting number of bedrooms as string\n",
    "def get_bedrooms(soup):\n",
    "    try:\n",
    "        home_prop_info = soup.find_all(\"div\", {'class': 'text-block__body'})[0].find_all(\"div\", {'class': 'overview__column'})\n",
    "        bed_rooms =home_prop_info[0].find_all(\"div\", {'class': 'overview__item'})[0].find_all('span', {'class':'overview__text'})[0].text.strip()\n",
    "        bed_rooms = re.search(r'\\d+', bed_rooms)\n",
    "        return str(bed_rooms.group()) if bed_rooms else None    # convert to str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in bed_rooms : {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting Living area size  as string\n",
    "def get_living_area(soup):\n",
    "    try:\n",
    "        home_prop_info = soup.find_all(\"div\", {'class': 'text-block__body'})[0].find_all(\"div\", {'class': 'overview__column'})\n",
    "        space = home_prop_info[1].find_all(\"div\", {'class': 'overview__item'})[0].find_all('span', {'class':'overview__text'})[0].text.strip()\n",
    "        space = re.findall(r'\\d+', space)[0]\n",
    "        return str(space)  # convert to str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in space: {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting kitchen type\n",
    "def get_kitchen(soup):\n",
    "    try:\n",
    "        kitchen_keywords = ('Kitchen type', 'Type of kitchen')\n",
    "        kitchen_th = soup.find('th', string=lambda x: x and x.strip() in kitchen_keywords)\n",
    "        if kitchen_th:\n",
    "            kitchen = kitchen_th.find_next_sibling('td').contents[0].strip()\n",
    "            return 1 if kitchen in ('Installed','Installed', 'Hyper equipped', 'USA  Hyper equipped','Semi equipped','USA hyper equipped') else 0\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in kitchen: {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting state of building\n",
    "def get_building_condition(soup):\n",
    "    try:\n",
    "        building_condition_header = soup.find('th', string=lambda x: x and x.strip() == 'Building condition').find_parent('tr')\n",
    "        return building_condition_header.find('td', class_='classified-table__data').contents[0].strip()\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in building_condition: {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting number of frontages (facades) as string\n",
    "def get_number_of_facades(soup):\n",
    "    try:\n",
    "        facade_keywords = re.compile(r'Number of (frontages|facades)', re.IGNORECASE)\n",
    "        facades_th = soup.find('th', string=facade_keywords)\n",
    "        return str(facades_th.find_next_sibling('td').contents[0].strip()) if facades_th else None  # convert to str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in Number_of_facades : {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting furnished or not\n",
    "def get_furnished(soup):\n",
    "    try:\n",
    "        Furnished = soup.find('th', string=lambda x: x and x.strip() == 'Furnished').find_next_sibling('td').contents[0].strip()\n",
    "        return 1 if Furnished == 'Yes' else 0\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in Furnished: {e}\")\n",
    "        return 0\n",
    "\n",
    "# extracting fire place state\n",
    "def get_open_fire(soup):\n",
    "    try:\n",
    "        Open_fire = soup.find('th', string=lambda x: x and x.strip() == 'How many fireplaces?').find_next_sibling('td').contents[0].strip()\n",
    "        return 1 if Open_fire else 0\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in Open_fire: {e}\") \n",
    "        return 0\n",
    "\n",
    "# extracting swimming pool state\n",
    "def get_swimming_pool(soup):\n",
    "    try:\n",
    "        Swimming_pool = soup.find('th', string=lambda text: text and 'Swimming pool' in text.strip()).find_next_sibling('td').contents[0].strip()\n",
    "        return 1 if Swimming_pool == 'Yes' else 0\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in Swimming_pool: {e}\")\n",
    "        return 0\n",
    "\n",
    "# extracting garden state and garden size as string\n",
    "def get_garden(soup):\n",
    "    try:\n",
    "        garden = soup.find('th', string=re.compile(r'^Garden.*')).find_next_sibling('td').contents[0].strip()\n",
    "        return str(garden) if garden else None  # convert to str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in garden: {e}\")\n",
    "        return None\n",
    "\n",
    "# extracting trass status and trass size as string\n",
    "def get_terrace(soup):\n",
    "    try:\n",
    "        Terrace = soup.find('th', string=re.compile(r'^Terrace.*')).find_next_sibling('td').contents[0].strip()\n",
    "        return str(Terrace) if Terrace else None  # convert to str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in Terrace: {e}\")\n",
    "        return None\n",
    "    \n",
    "# extracting public or online sale\n",
    "def get_sale_type(soup):\n",
    "    try:\n",
    "        sale_keywords = ['public sale', 'online sale']       # looking for \"public sale\" \"online sale\" \n",
    "        sale_text = soup.find('body').text.lower()\n",
    "        for keyword in sale_keywords:\n",
    "            if keyword in sale_text:\n",
    "                return keyword           # return it if its for onlune sale\n",
    "        return None                       # if note, none\n",
    "    except Exception as e:\n",
    "        #print(f\"Error in sale_type: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_sale_type(soup):\n",
    "    try:\n",
    "        sale_section = soup.find('span', class_='text-block__subtitle')      \n",
    "        if sale_section:\n",
    "            sale_text = sale_section.text.strip()  # remove spaces \n",
    "            if sale_text in ['Public sale', 'Online sale']:  # compare with text\n",
    "                return sale_text       \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# calling all functions\n",
    "def request_url(url):\n",
    "    soup = get_soup(url)\n",
    "    html_list = soup.find(\"meta\", {'property': \"og:url\"}).get('content').split(\"/\")\n",
    "    \n",
    "    property_details = {\n",
    "        'Property ID': get_property_id(soup),\n",
    "        'Postal code': get_postal_code(html_list),\n",
    "        'Locality name': get_locality(html_list),\n",
    "        'Price': get_price(soup),\n",
    "        'Type of property': get_property_type(html_list),\n",
    "        'Number of rooms': get_bedrooms(soup),\n",
    "        'Living area': get_living_area(soup),\n",
    "        'Equipped kitchen': get_kitchen(soup),\n",
    "        'State of building': get_building_condition(soup),\n",
    "        'Number of facades': get_number_of_facades(soup),\n",
    "        'Furnished': get_furnished(soup),\n",
    "        'Open fire': get_open_fire(soup),\n",
    "        'Swimming pool': get_swimming_pool(soup),\n",
    "        'Garden (m²)': get_garden(soup),\n",
    "        'Terrace (m²)': get_terrace(soup),\n",
    "        'Sale type': get_sale_type(soup)  # sale type\n",
    "    }\n",
    "\n",
    "    return property_details\n",
    "\n",
    "#_____________________________________________________________________________________________\n",
    "#  calling and saving\n",
    "\n",
    "\n",
    "# reading input file (linkes)\n",
    "def read_input_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df['URL'].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# process the links and extract the details of sale items\n",
    "def process_urls(url_list):\n",
    "    property_details = []\n",
    "    for url in url_list:\n",
    "        property_details.append(request_url(url))\n",
    "    return property_details\n",
    "\n",
    "# saving the details of sale items\n",
    "# def write_output_file(file_path, property_details):\n",
    "#     try:\n",
    "#         df_properties = pd.DataFrame(property_details)\n",
    "#         df_properties.to_csv(file_path, index=False, encoding='utf-8')\n",
    "#         print(f\"CSV file created: {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error writing to file {file_path}: {e}\")\n",
    "\n",
    "# saving the details of sale items\n",
    "def write_output_file(file_path, property_details):\n",
    "    try:\n",
    "        df_properties = pd.DataFrame(property_details)\n",
    "        \n",
    "        print(df_properties.head())\n",
    "        \n",
    "        df_properties.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        print(f\"CSV file created: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file {file_path}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process all files\n",
    "def process_files(input_file_template, output_file_path):   # calling to save all details in only 1 file\n",
    "    url_list = read_input_file(input_file_template)  # read the files from linkes \n",
    "    property_details = process_urls(url_list)   # take the links and extract the details  \n",
    "    write_output_file(output_file_path, property_details)   # save all details in a csv file\n",
    "\n",
    "\n",
    "# def process_files(num_files, input_file_template, output_file_template):\n",
    "#     for i in range(1, num_files + 1):  # Loop from 1 to num_files\n",
    "#         input_file_path = input_file_template.format(i)\n",
    "#         output_file_path = output_file_template.format(i)      \n",
    "#         # reading the input file and extract the links\n",
    "#         url_list = read_input_file(input_file_path)       \n",
    "#         # process the links and extract the detail of items\n",
    "#         property_details = process_urls(url_list)       \n",
    "#         # save the extracted detail in output file\n",
    "#         write_output_file(output_file_path, property_details)\n",
    "\n",
    "# setting the paths and number of files\n",
    "input_file_path = \"C:\\\\Users\\\\becod\\\\AI\\\\my-projects\\\\Majid_immoeliza_cleanup\\\\Links\\\\all_links.csv\"\n",
    "output_file_path_template = \"C:\\\\Users\\\\becod\\\\AI\\\\my-projects\\\\Majid_immoeliza_cleanup\\\\Details\\\\property_details.csv\"\n",
    "\n",
    "# calling the main function \n",
    "process_files(input_file_path, output_file_path_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\Users\\becod\\AI\\my-projects\\Majid_immoeliza_cleanup\\Cleaned\\all_results_cleanedup.csv\n"
     ]
    }
   ],
   "source": [
    "# cleaning\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# read CSV file\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# 1. remove duplicate rows\n",
    "def remove_duplicates(data):\n",
    "    return data.drop_duplicates()\n",
    "\n",
    "# 2. remove properties with duplicate 'Property ID' but different values\n",
    "def remove_duplicate_properties(data):\n",
    "    duplicate_properties = data.groupby('Property ID').filter(lambda x: len(x) > 1)\n",
    "    return data[~data['Property ID'].isin(duplicate_properties['Property ID'])]\n",
    "\n",
    "# 3.1. clean 'Locality name' column by removing extra characters\n",
    "def clean_locality_name(name):\n",
    "    name = name.replace('%C', 'e')               # replace  %C with  e (it was originally è)\n",
    "    name = name.replace('%27', \"'\")              # replace  %27 with  '\n",
    "    name = name.replace('%20', ' ')              # replace  %20 with  space\n",
    "    name = re.sub(r'%20%28.*?%29', '', name)     # remove characters between %20%28 and %29 \n",
    "    name = re.sub(r'[0-9\\(\\)%]', '', name)       # remove any numbers and special characters (like parenthes and %)\n",
    "    name = re.sub(r'-\\d+$', '', name)            # remove numbers and - if a city name ended with (-number)\n",
    "    name = name.lower().title()                  # convert to lowercase and capitalize the first letter of each word # for some cities with several word name \n",
    "    return name.strip()                          # remove any extra spaces\n",
    "\n",
    "# 3.2. Apply the cleaning function to the 'Locality name' column in the dataset\n",
    "def clean_locality_names_column(df):\n",
    "    df['Locality name'] = df['Locality name'].apply(clean_locality_name)\n",
    "    return df\n",
    "\n",
    "# 4.remove rows if 'Sale type' value is 'online sale' or 'public sale'\n",
    "def remove_sale_type(data):\n",
    "    return data[~data['Sale type'].isin(['online sale', 'public sale'])]\n",
    "\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# save the cleaned DataFrame to a CSV file\n",
    "def save_to_csv(data, output_path):\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# main function to orchestrate the cleaning process\n",
    "def main():\n",
    "    file_path = 'C:\\\\Users\\\\becod\\\\AI\\\\my-projects\\\\Majid_immoeliza_cleanup\\\\Details\\\\property_details.csv'\n",
    "  \n",
    "    output_file_path = 'C:\\\\Users\\\\becod\\\\AI\\\\my-projects\\\\Majid_immoeliza_cleanup\\\\Cleaned\\\\all_results_cleanedup.csv'\n",
    "    \n",
    "    # Read the CSV file\n",
    "    data = read_csv(file_path)\n",
    "    cd\n",
    "    data_final = remove_sale_type(data_final)\n",
    "    \n",
    "    # Save the cleaned data to a new CSV\n",
    "    save_to_csv(data_final, output_file_path)\n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تمیزکاری داده‌ها\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# خواندن فایل CSV\n",
    "def read_csv(file_path):\n",
    "    # اینجا از کتابخانه pandas برای خواندن فایل CSV و بارگذاری آن در یک DataFrame استفاده شده است\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# 1. حذف ردیف‌های تکراری\n",
    "def remove_duplicates(data):\n",
    "    # اینجا از متد drop_duplicates در pandas برای حذف ردیف‌های تکراری از DataFrame استفاده شده است\n",
    "    return data.drop_duplicates()\n",
    "\n",
    "# 2. حذف املاک با 'Property ID' تکراری اما مقادیر متفاوت\n",
    "def remove_duplicate_properties(data):\n",
    "    # اینجا از متد groupby و filter در pandas برای پیدا کردن 'Property ID' های تکراری استفاده شده است\n",
    "    # همچنین از isin و ایندکس‌گذاری pandas برای فیلتر کردن DataFrame استفاده شده است\n",
    "    duplicate_properties = data.groupby('Property ID').filter(lambda x: len(x) > 1)\n",
    "    return data[~data['Property ID'].isin(duplicate_properties['Property ID'])]\n",
    "\n",
    "# 3.1. پاکسازی ستون 'Locality name' با حذف کاراکترهای اضافی\n",
    "def clean_locality_name(name):\n",
    "    # اینجا از pandas به طور مستقیم استفاده نشده است، اما این تابع بعداً بر روی یک ستون pandas اعمال می‌شود\n",
    "    name = name.replace('%C', 'e')               # جایگزینی %C با e (اصالتاً è بود)\n",
    "    name = name.replace('%27', \"'\")              # جایگزینی %27 با '\n",
    "    name = name.replace('%20', ' ')              # جایگزینی %20 با فاصله\n",
    "    name = re.sub(r'%20%28.*?%29', '', name)     # حذف کاراکترها بین %20%28 و %29\n",
    "    name = re.sub(r'[0-9\\(\\)%]', '', name)       # حذف هر عدد و کاراکترهای خاص مانند پرانتز و %\n",
    "    name = re.sub(r'-\\d+$', '', name)            # حذف اعداد و - اگر نام شهر با (-عدد) تمام شود\n",
    "    name = name.lower().title()                  # تبدیل به حروف کوچک و سپس بزرگ کردن اولین حرف هر کلمه\n",
    "    return name.strip()                          # حذف فاصله‌های اضافی\n",
    "\n",
    "# 3.2. اعمال تابع پاکسازی بر روی ستون 'Locality name' در دیتاست\n",
    "def clean_locality_names_column(df):\n",
    "    # اینجا از متد apply در pandas برای اعمال تابع پاکسازی بر روی ستون 'Locality name' استفاده شده است\n",
    "    df['Locality name'] = df['Locality name'].apply(clean_locality_name)\n",
    "    return df\n",
    "\n",
    "# 4. حذف ردیف‌ها اگر مقدار 'Sale type' برابر با 'online sale' یا 'public sale' باشد\n",
    "def remove_sale_type(data):\n",
    "    # اینجا از متد isin در pandas برای فیلتر کردن ردیف‌هایی که 'Sale type' برابر با 'online sale' یا 'public sale' است استفاده شده است\n",
    "    return data[~data['Sale type'].isin(['online sale', 'public sale'])]\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# ذخیره کردن DataFrame تمیز شده در یک فایل CSV\n",
    "def save_to_csv(data, output_path):\n",
    "    # اینجا از متد to_csv در pandas برای ذخیره DataFrame به یک فایل CSV استفاده شده است\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# __________________________________________________________________________\n",
    "\n",
    "# تابع اصلی برای مدیریت فرآیند تمیزکاری\n",
    "def main():\n",
    "    file_path = 'C:\\\\Users\\\\becod\\\\AI\\\\my-projects\\\\Majid_immoeliza_cleanup\\\\Details\\\\property_details.csv'\n",
    "    output_file_path = 'C:\\\\Users\\\\becod\\\\AI\\\\my-projects\\\\Majid_immoeliza_cleanup\\\\Cleaned\\\\all_results_cleanedup.csv'\n",
    "    \n",
    "    # 1. خواندن فایل CSV به یک DataFrame\n",
    "    # از متد read_csv pandas استفاده شده است\n",
    "    data = read_csv(file_path)\n",
    "    \n",
    "    # 2. حذف ردیف‌های تکراری با استفاده از متد drop_duplicates pandas\n",
    "    data_cleaned = remove_duplicates(data)\n",
    "    \n",
    "    # 3. حذف املاک با 'Property ID' تکراری اما مقادیر متفاوت با استفاده از متد groupby، filter، isin و فیلتر کردن در pandas\n",
    "    data_final = remove_duplicate_properties(data_cleaned)\n",
    "    \n",
    "    # 4. پاکسازی ستون 'Locality name' با اعمال تابع تمیزسازی با استفاده از متد apply pandas\n",
    "    data_final = clean_locality_names_column(data_final)\n",
    "\n",
    "    # 5. حذف ردیف‌ها با 'online sale' یا 'public sale' در 'Sale type' با استفاده از متد isin pandas\n",
    "    data_final = remove_sale_type(data_final)\n",
    "    \n",
    "    # 6. ذخیره DataFrame تمیز شده به یک فایل CSV جدید با استفاده از متد to_csv pandas\n",
    "    save_to_csv(data_final, output_file_path)\n",
    "\n",
    "# فراخوانی تابع اصلی\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
